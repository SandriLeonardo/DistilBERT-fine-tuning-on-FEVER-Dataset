{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer, DataCollatorWithPadding\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "releasing memory allocated on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the model you wish to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#                                                     language_model_name,\n",
    "#                                                     num_labels=3,        # number of categories\n",
    "#                                                     ignore_mismatched_sizes=True,\n",
    "#                                                     output_attentions=False,\n",
    "#                                                     output_hidden_states=False,\n",
    "#                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_val = AutoModelForSequenceClassification.from_pretrained(\n",
    "#                                                     language_model_name,\n",
    "#                                                     num_labels=3,        # number of categories\n",
    "#                                                     ignore_mismatched_sizes=True,\n",
    "#                                                     output_attentions=False,\n",
    "#                                                     output_hidden_states=False,\n",
    "#                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_wsd = AutoModelForSequenceClassification.from_pretrained(\n",
    "#                                                     language_model_name,\n",
    "#                                                     num_labels=3,        # number of categories\n",
    "#                                                     ignore_mismatched_sizes=True,\n",
    "#                                                     output_attentions=False,\n",
    "#                                                     output_hidden_states=False,\n",
    "#                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_srl = AutoModelForSequenceClassification.from_pretrained(\n",
    "#                                                     language_model_name,\n",
    "#                                                     num_labels=3,        # number of categories\n",
    "#                                                     ignore_mismatched_sizes=True,\n",
    "#                                                     output_attentions=False,\n",
    "#                                                     output_hidden_states=False,\n",
    "#                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_srl_wsd = AutoModelForSequenceClassification.from_pretrained(\n",
    "#                                                     language_model_name,\n",
    "#                                                     num_labels=3,        # number of categories\n",
    "#                                                     ignore_mismatched_sizes=True,\n",
    "#                                                     output_attentions=False,\n",
    "#                                                     output_hidden_states=False,\n",
    "#                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the already trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_val = AutoModelForSequenceClassification.from_pretrained(\n",
    "                                                    './weights/valTrial',\n",
    "                                                    num_labels=3,        # number of categories\n",
    "                                                    ignore_mismatched_sizes=True,\n",
    "                                                    output_attentions=False,\n",
    "                                                    output_hidden_states=False,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wsd = AutoModelForSequenceClassification.from_pretrained(\n",
    "                                                    './weights/wsdTrial',\n",
    "                                                    num_labels=3,        # number of categories\n",
    "                                                    ignore_mismatched_sizes=True,\n",
    "                                                    output_attentions=False,\n",
    "                                                    output_hidden_states=False,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_srl = AutoModelForSequenceClassification.from_pretrained(\n",
    "                                                    './weights/srlTrial',\n",
    "                                                    num_labels=3,        # number of categories\n",
    "                                                    ignore_mismatched_sizes=True,\n",
    "                                                    output_attentions=False,\n",
    "                                                    output_hidden_states=False,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_srl_wsd = AutoModelForSequenceClassification.from_pretrained(\n",
    "                                                    './weights/srl_wsd_Trial',\n",
    "                                                    num_labels=3,        # number of categories\n",
    "                                                    ignore_mismatched_sizes=True,\n",
    "                                                    output_attentions=False,\n",
    "                                                    output_hidden_states=False,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "                        path = 'parquet',\n",
    "                        data_files = {\n",
    "                                    'train': 'data/train-00000-of-00001.parquet',\n",
    "                                    'validation' : 'data/validation-00000-of-00001.parquet'\n",
    "                                },\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_test_set = datasets.load_dataset(\n",
    "                        path = 'parquet',\n",
    "                        data_files =  {\n",
    "                            'test' : 'data/adv-test-00000-of-00001.parquet'\n",
    "                        }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd_aug_train_dataset = datasets.load_dataset(\n",
    "                        path = 'parquet',\n",
    "                        data_files =  {\n",
    "                            'train' : 'data/wsd_aug_train_dataset.parquet'\n",
    "                        }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd_aug_val_dataset = datasets.load_dataset(\n",
    "                        path = 'parquet',\n",
    "                        data_files =  {\n",
    "                            'validation' : 'data/wsd_aug_val_dataset.parquet'\n",
    "                        }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_aug_train_dataset = datasets.load_dataset(\n",
    "                        path = 'parquet',\n",
    "                        data_files = {\n",
    "                            'train' : 'data/srl_aug_train_dataset.parquet'\n",
    "                        }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_aug_val_dataset = datasets.load_dataset(\n",
    "                        path = 'parquet',\n",
    "                        data_files =  {\n",
    "                            'validation' : 'data/srl_aug_val_dataset.parquet'\n",
    "                        }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_wsd_aug_train_dataset = datasets.load_dataset(\n",
    "                        path = 'parquet',\n",
    "                        data_files =  {\n",
    "                            'train' : 'data/srl_wsd_aug_train_dataset.parquet'\n",
    "                        }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_wsd_aug_val_dataset = datasets.load_dataset(\n",
    "                        path = 'parquet',\n",
    "                        data_files = {\n",
    "                            'validation' : 'data/srl_wsd_aug_val_dataset.parquet'\n",
    "                        }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(language_model_name)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main mapping function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the dataset for the NLI task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLImapping(sample):\n",
    "    NLIsample = {}\n",
    "    NLIsample['id'] = sample['id']\n",
    "    if sample['label'] == 'ENTAILMENT':\n",
    "        NLIsample['label'] = 0\n",
    "    elif sample['label'] == 'NEUTRAL':\n",
    "        NLIsample['label'] = 1\n",
    "    else:\n",
    "        NLIsample['label'] = 2\n",
    "\n",
    "    NLIsample['sentence'] = '[CLS] ' + sample['premise'] + ' [SEP] ' + sample['hypothesis']\n",
    "    return NLIsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEVER dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fever_dataset = dataset.remove_columns(['wsd', 'srl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLIDataset = Fever_dataset.map(NLImapping, remove_columns=['premise', 'hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_FEVER_datasets = NLIDataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Adversarial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advNLImapping(sample):\n",
    "    NLIsample = {}\n",
    "    NLIsample['id'] = sample['cid']\n",
    "    if sample['label'] == 'ENTAILMENT':\n",
    "        NLIsample['label'] = 0\n",
    "    elif sample['label'] == 'NEUTRAL':\n",
    "        NLIsample['label'] = 1\n",
    "    else:\n",
    "        NLIsample['label'] = 2\n",
    "\n",
    "    NLIsample['sentence'] = '[CLS] ' + sample['premise'] + ' [SEP] ' + sample['hypothesis']\n",
    "    return NLIsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advNLIDataset = adv_test_set.map(advNLImapping, remove_columns=['premise', 'hypothesis', 'cid','part'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_adv = advNLIDataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WSD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd_aug_train_dataset_NLI = wsd_aug_train_dataset.map(NLImapping, remove_columns=['premise', 'hypothesis'])\n",
    "wsd_aug_val_dataset_NLI = wsd_aug_val_dataset.map(NLImapping, remove_columns=['premise', 'hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_wsd_aug_train = wsd_aug_train_dataset_NLI.map(tokenize_function, batched=True)\n",
    "tokenized_wsd_aug_val = wsd_aug_val_dataset_NLI.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SRL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_aug_train_datasetNLI = srl_aug_train_dataset.map(NLImapping, remove_columns=['premise', 'hypothesis'])\n",
    "srl_aug_val_datasetNLI = srl_aug_val_dataset.map(NLImapping, remove_columns=['premise', 'hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_srl_aug_train = srl_aug_train_datasetNLI.map(tokenize_function, batched=True)\n",
    "tokenized_srl_aug_val = srl_aug_val_datasetNLI.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WSD + SRL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_wsd_aug_train_datasetNLI = srl_wsd_aug_train_dataset.map(NLImapping, remove_columns=['premise', 'hypothesis'])\n",
    "srl_wsd_aug_val_datasetNLI = srl_wsd_aug_val_dataset.map(NLImapping, remove_columns=['premise', 'hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_srl_wsd_aug_train = srl_wsd_aug_train_datasetNLI.map(tokenize_function, batched=True)\n",
    "tokenized_srl_wsd_aug_val = srl_wsd_aug_val_datasetNLI.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training common settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"training_dir\",                    # output directory [Mandatory]\n",
    "    num_train_epochs=2,                           # total number of training epochs\n",
    "    per_device_train_batch_size=8,                # batch size per device during training\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,                             # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.004,                           # strength of weight decay\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",                  # sets the evaluation to happen every certain number of steps.\n",
    "    eval_steps=500,                               # specifies that evaluation should happen every 500 steps\n",
    "    save_steps=500,                               # saves the model checkpoint every 500 steps\n",
    "    load_best_model_at_end=True,                  # ensures that the best model (based on the metric specified) is loaded at the end of training\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    learning_rate=1e-5,                           # learning rate\n",
    "    lr_scheduler_type = 'linear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = datasets.load_metric(\"accuracy\")\n",
    "   load_f1 = datasets.load_metric(\"f1\")\n",
    "\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels, average='macro')[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the FEVER Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_val = Trainer(\n",
    "   model=model_val,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_FEVER_datasets[\"train\"],\n",
    "   eval_dataset=tokenized_FEVER_datasets[\"validation\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment below if you want to train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer_val.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment below to save the weights of the just trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('./newTrial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on the FEVER validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer_val.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on the Adversarial Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_adv = Trainer(\n",
    "   model=model_val,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_FEVER_datasets[\"train\"],\n",
    "   eval_dataset=tokenized_adv[\"test\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_adv_results = trainer_adv.evaluate()\n",
    "print(eval_adv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and comparison between evaluation on FEVER and Adversarial set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracy = eval_results['eval_accuracy']\n",
    "adv_accuracy = eval_adv_results['eval_accuracy']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar('val_acc', val_accuracy, color='blue', label='val_acc')\n",
    "ax.bar('adv_acc', adv_accuracy, color='red', label='adv_acc')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Comparison of val_acc and adv_acc')\n",
    "\n",
    "ax.legend()\n",
    "plt.savefig('2comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on the WSD augmented dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_wsd = Trainer(\n",
    "   model=model_wsd,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_wsd_aug_train['train'],\n",
    "   eval_dataset=tokenized_wsd_aug_val['validation'],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment below in order to train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer_wsd.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unccoment below to save the weights of the just trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_wsd.save_pretrained('./wsdTrial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model_wsd on the FEVER validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained on the WSD augmented dataset is evaluated on the FEVER validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_syn_wsd_results = trainer_wsd.evaluate()\n",
    "print(eval_syn_wsd_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model_wsd on the adversarial test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained on the wsd augmented dataset is evaluated on the adversarial test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_adv_wsd = Trainer(\n",
    "   model=model_wsd,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_wsd_aug_train['train'],\n",
    "   eval_dataset=tokenized_adv[\"test\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_adv_wsd_results = trainer_adv_wsd.evaluate()\n",
    "print(eval_adv_wsd_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot comparison between evaluation on FEVER, Adversarial with base model and Adversarial set with model_wsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracy = eval_results['eval_accuracy']\n",
    "adv_accuracy = eval_adv_results['eval_accuracy']\n",
    "wsd_accuracy = eval_adv_wsd_results['eval_accuracy']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar('val_acc', val_accuracy, color='blue', label='val_acc')\n",
    "ax.bar('adv_acc', adv_accuracy, color='red', label='adv_acc')\n",
    "ax.bar('wsd_acc', wsd_accuracy, color='green', label='wsd_acc')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Comparison of val_acc, adv_acc and wsd_acc')\n",
    "\n",
    "ax.legend()\n",
    "# plt.savefig('comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on the SRL augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_srl = Trainer(\n",
    "   model=model_srl,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_srl_aug_train['train'],\n",
    "   eval_dataset=tokenized_srl_aug_val['validation'],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer_srl.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to save the weights of the just trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_srl.save_pretrained('./srlTrial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model_srl on the FEVER validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_srl_results = trainer_srl.evaluate()\n",
    "print(eval_srl_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model_srl on the adversarial validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_srl_adv = Trainer(\n",
    "    model=model_srl,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_srl_aug_train['train'],\n",
    "    eval_dataset=tokenized_adv[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_srl_adv_results = trainer_srl_adv.evaluate()\n",
    "print(eval_srl_adv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on the SRL + WSD augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_srl_wsd = Trainer(\n",
    "    model=model_srl_wsd,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_srl_wsd_aug_train['train'],\n",
    "    eval_dataset=tokenized_srl_wsd_aug_val['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer_srl_wsd.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to save the weights of the just trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_srl_wsd.save_pretrained('./srl_wsd_Trial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model_srl_wsd on the FEVER validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_srl_wsd_results = trainer_srl_wsd.evaluate()\n",
    "print(eval_srl_wsd_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model_srl_wsd on the adversarial set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_srl_wsd_adv = Trainer(\n",
    "    model=model_srl_wsd,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_srl_wsd_aug_train['train'],\n",
    "    eval_dataset=tokenized_adv[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_srl_wsd_results_adv = trainer_srl_wsd_adv.evaluate()\n",
    "print(eval_srl_wsd_results_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot comparison between evaluation on:\n",
    "- Adversarial with base model \n",
    "- Adversarial set with model_wsd \n",
    "- Adversarial set with model_srl \n",
    "- Adversarial set with model_srl_wsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_accuracy = eval_adv_results['eval_accuracy']\n",
    "wsd_accuracy = eval_adv_wsd_results['eval_accuracy']\n",
    "srl_accuracy = eval_srl_adv_results['eval_accuracy']\n",
    "srl_wsd_accuracy = eval_srl_wsd_results_adv['eval_accuracy']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar('adv_acc', adv_accuracy, color='red', label='adv_acc')\n",
    "ax.bar('wsd_acc', wsd_accuracy, color='green', label='wsd_acc')\n",
    "ax.bar('srl_acc', srl_accuracy, color='yellow', label='srl_acc')\n",
    "ax.bar('srl_wsd_acc', srl_wsd_accuracy, color='purple', label='srl_wsd_acc')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Comparison of adv_acc, wsd_acc, srl_acc and wsd+srl_acc')\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "plt.savefig('4comparison.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NOME_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/leonardo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "releasing memory allocated on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "                        path = 'parquet',\n",
    "                        data_files = {\n",
    "                                    'train': 'data/train-00000-of-00001.parquet',\n",
    "                                    'validation' : 'data/validation-00000-of-00001.parquet'\n",
    "                                },\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WSD data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd_dataset = dataset.remove_columns(['srl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  substituting all synonyms for each word in each sample premise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms_by_offset(wn_synset_offset, original_word, index):\n",
    "    \"\"\"This function returns a list of synonyms for a given wordnet synset offset.\"\"\"\n",
    "    try:\n",
    "        if wn_synset_offset == 'O':\n",
    "            return []\n",
    "        \n",
    "        if wn_synset_offset[-1].isalpha() and wn_synset_offset[:-1].isdigit():\n",
    "            offset_str, pos_indicator = wn_synset_offset[:-1], wn_synset_offset[-1]\n",
    "        else: \n",
    "            raise ValueError(f\"wn_synset_offset is not in the correct format at id {index}.\")\n",
    "        \n",
    "        offset_int = int(offset_str)\n",
    "            \n",
    "        synset = wn.synset_from_pos_and_offset(pos_indicator, offset_int)\n",
    "        if synset is None:\n",
    "            return []\n",
    "        \n",
    "        synonyms = set(lemma.name().replace('_', ' ') for lemma in synset.lemmas() if lemma.name().lower() != original_word.lower())\n",
    "        return list(synonyms)\n",
    "    except Exception as e:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsd_mapping(sample):\n",
    "    \"\"\"This function create a new sample for each synonym of each word. If no synonym is found, the sample is discarded.\"\"\"\n",
    "    index = sample['id']\n",
    "    for word in sample['wsd']['premise']:\n",
    "        wn_synset_offset = word['wnSynsetOffset']\n",
    "        original_word = word['text']\n",
    "\n",
    "        if not wn_synset_offset or wn_synset_offset == 'O':\n",
    "            continue\n",
    "        else:\n",
    "            synonyms = get_synonyms_by_offset(wn_synset_offset, original_word, index)\n",
    "            if synonyms:\n",
    "                sample['premise'] = sample['premise'].replace(original_word, synonyms[0])\n",
    "            else:\n",
    "                continue\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_dataset = wsd_dataset.map(wsd_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd_aug_train_dataset = datasets.concatenate_datasets([wsd_dataset['train'], synonym_dataset['train']])\n",
    "wsd_aug_train_dataset = wsd_aug_train_dataset.remove_columns(['wsd'])\n",
    "\n",
    "wsd_aug_val_dataset = datasets.concatenate_datasets([wsd_dataset['validation'], synonym_dataset['validation']])\n",
    "wsd_aug_val_dataset = wsd_aug_val_dataset.remove_columns(['wsd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now save the augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 103/103 [00:00<00:00, 915.58ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 953.99ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1906335"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_aug_train_dataset.to_parquet('wsd_aug_train_dataset.parquet')\n",
    "wsd_aug_val_dataset.to_parquet('wsd_aug_val_dataset.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRL data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_dataset = dataset.remove_columns(['wsd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching agent and patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of samples with switched agent and patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switcherSRL(dataset):\n",
    "\n",
    "    \"\"\"This function creates new samples that are used to generate a new dataset in order to augment the starting one. \n",
    "    The new samples are created by switching the position of the agent and the patient in the premise of the original samples.\n",
    "    Spans corresponding to the agent and the patient are extracted from the SRL annotations and used to switch the words in the premise.\n",
    "    Which is done by creating a new premise from scratch.\n",
    "    Also samples missing both roles (agent and patient) are discarded and counted in order to quantify the loss of samples.\"\"\"\n",
    "    \n",
    "    new_samples = []\n",
    "    missing_sample_count = 0\n",
    "\n",
    "    for sample in dataset:\n",
    "        agent_span = []\n",
    "        patient_span = []\n",
    "        words = []\n",
    "\n",
    "        # spans extraction\n",
    "        for annotation in sample['srl']['premise'].get('annotations', []):\n",
    "            if 'verbatlas' in annotation:\n",
    "                for role in annotation['verbatlas']['roles']:\n",
    "                    if role['role'] == 'Agent':\n",
    "                        agent_span = role['span']\n",
    "                    if role['role'] == 'Patient':\n",
    "                        patient_span = role['span']\n",
    "\n",
    "        # words extraction\n",
    "        for token in sample['srl']['premise'].get('tokens', []):\n",
    "            words.append(token['rawText'])\n",
    "\n",
    "        # New premise creation\n",
    "        if agent_span and patient_span:\n",
    "            agent_words = words[agent_span[0]:agent_span[1]+1]\n",
    "            patient_words = words[patient_span[0]:patient_span[1]+1]\n",
    "            \n",
    "            first_span, second_span = sorted([agent_span, patient_span], key=lambda x: x[0])\n",
    "            first_words, second_words = (agent_words, patient_words) if first_span == agent_span else (patient_words, agent_words)\n",
    "            \n",
    "            before_first_span = words[:first_span[0]]\n",
    "            between_spans = words[first_span[1]+1:second_span[0]]\n",
    "            after_second_span = words[second_span[1]+1:]\n",
    "            \n",
    "            if first_span == agent_span:\n",
    "                new_words = before_first_span + second_words + between_spans + first_words + after_second_span\n",
    "            else:\n",
    "                new_words = before_first_span + first_words + between_spans + second_words + after_second_span\n",
    "            \n",
    "            new_premise = ' '.join(new_words)\n",
    "            \n",
    "            new_sample = {\n",
    "                'id': sample['id'],\n",
    "                'premise': new_premise,\n",
    "                'hypothesis': sample['hypothesis'],\n",
    "                'label': sample['label']\n",
    "            }\n",
    "            new_samples.append(new_sample)\n",
    "        \n",
    "        # missing samples count\n",
    "        else:\n",
    "            missing_sample_count += 1\n",
    "\n",
    "    print(f\"Missing roles in {missing_sample_count} samples.\")\n",
    "    return new_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing roles in 29700 samples.\n",
      "Missing roles in 1455 samples.\n"
     ]
    }
   ],
   "source": [
    "switched_srl_train = switcherSRL(srl_dataset['train'])\n",
    "switched_srl_val = switcherSRL(srl_dataset['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In order to create a new dataset, the samples obtained from the switcher function are stored in dictionaries.\"\"\"\n",
    "\n",
    "dataset_dict_srl_train = {\n",
    "    'id': [sample['id'] for sample in switched_srl_train],\n",
    "    'premise': [sample['premise'] for sample in switched_srl_train],\n",
    "    'hypothesis': [sample['hypothesis'] for sample in switched_srl_train],\n",
    "    'label': [sample['label'] for sample in switched_srl_train]\n",
    "}\n",
    "\n",
    "dataset_dict_srl_val = {\n",
    "    'id': [sample['id'] for sample in switched_srl_val],\n",
    "    'premise': [sample['premise'] for sample in switched_srl_val],\n",
    "    'hypothesis': [sample['hypothesis'] for sample in switched_srl_val],\n",
    "    'label': [sample['label'] for sample in switched_srl_val]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "switched_srl_train_dataset = datasets.Dataset.from_dict(dataset_dict_srl_train)\n",
    "switched_srl_val_dataset = datasets.Dataset.from_dict(dataset_dict_srl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_dataset = srl_dataset.remove_columns(['srl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_aug_train_dataset = datasets.concatenate_datasets([srl_dataset['train'], switched_srl_train_dataset])\n",
    "srl_aug_val_dataset = datasets.concatenate_datasets([srl_dataset['validation'], switched_srl_val_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 73/73 [00:00<00:00, 923.99ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 1042.84ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1367667"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srl_aug_train_dataset.to_parquet('srl_aug_train_dataset.parquet')\n",
    "srl_aug_val_dataset.to_parquet('srl_aug_val_dataset.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRL + WSD data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_wsd_aug_train_dataset = datasets.concatenate_datasets([switched_srl_train_dataset, wsd_aug_train_dataset])\n",
    "srl_wsd_aug_val_dataset = datasets.concatenate_datasets([switched_srl_val_dataset, wsd_aug_val_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 124/124 [00:00<00:00, 830.26ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 962.00ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2356278"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srl_wsd_aug_train_dataset.to_parquet('srl_wsd_aug_train_dataset.parquet')\n",
    "srl_wsd_aug_val_dataset.to_parquet('srl_wsd_aug_val_dataset.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NOME_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
